{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EmergencySystem.ipynb","provenance":[],"collapsed_sections":["rjYVcKL5dm5K","dNlqbqV7_z1o","jd8xEI5pl60z","8g0dya8oo3uj","QWp-vip1pD0K","7TN5tXLKqxQM","Emk-Rmb9rqk_","xLZ2ZZzWr0y1"],"authorship_tag":"ABX9TyP53UPqbiwO7sTJ/unbqkPg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rjYVcKL5dm5K","colab_type":"text"},"source":["# MOUNT The Drive"]},{"cell_type":"code","metadata":{"id":"HydPh7KQgD2n","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dNlqbqV7_z1o","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"qKUMmlCV_y7l","colab_type":"code","colab":{}},"source":["def preprocessing(filename):\n","  video_path = filename\n","\n","  img_arr=[]\n","  cap = cv2.VideoCapture(video_path)\n","  success, img = cap.read()\n","  idx = 0\n","  num=1\n","  while success:\n","    idx+=1\n","    if num==idx:\n","      img_arr.append(img)\n","      num+=6\n","    success, img = cap.read()\n","\n","  height, width, layers = img_arr[0].shape\n","  size = (width,height)\n","\n","  out = cv2.VideoWriter('preprocess.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n","\n","  for i in range(len(img_arr)):\n","      out.write(img_arr[i])\n","\n","  out.release()\n","  cv2.destroyAllWindows()  \n","  \n","  result_path = \"/content/preprocess.avi\"\n","\n","  return  img_arr, result_path \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jd8xEI5pl60z","colab_type":"text"},"source":["# Violence preparation\n"]},{"cell_type":"code","metadata":{"id":"G_JRqpDpn9No","colab_type":"code","colab":{}},"source":["import sys\n","import cv2\n","import os\n","import numpy as np\n","import keras\n","from keras.models import model_from_json\n","from keras import backend as K\n","\n","\n","\n","path = \"/content/vid2\"\n","os.mkdir(path)\n","\n","\n","# load json and create model\n","json_file = open('/content/drive/My Drive/Emergency project/ViolenceDetection/pretrained/VGG16_Edited_model.json', 'r')\n","loaded_image_model_transfer_json = json_file.read()\n","json_file.close()\n","loaded_image_model_transfer = model_from_json(loaded_image_model_transfer_json)\n","# load weights into new model\n","loaded_image_model_transfer.load_weights(\"/content/drive/My Drive/Emergency project/ViolenceDetection/pretrained/VGG16_Edited_model.h5\")\n","print(\"Loaded model from disk\")\n","\n","\n","\n","# load json and create model\n","json_file = open('/content/drive/My Drive/Emergency project/ViolenceDetection/pretrained/violence_model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","# load weights into new model\n","loaded_model.load_weights(\"/content/drive/My Drive/Emergency project/ViolenceDetection/pretrained/violence_model.h5\")\n","print(\"Loaded model from disk\")\n","\n","\n","# # Frame size  \n","img_size = 224\n","\n","img_size_touple = (img_size, img_size)\n","\n","# # Number of channels (RGB)\n","num_channels = 3\n","\n","\n","# # classification-layer which is named fc2. This is a fully-connected (or dense) layer.\n","transfer_layer = loaded_image_model_transfer.get_layer('fc2')\n","\n","transfer_values_size = K.int_shape(transfer_layer.output)[1]\n","\n","\n","_images_per_file = 20\n","\n","\n","def get_frames(current_dir, file_name):\n","    \n","    in_file = os.path.join(current_dir, file_name)\n","    \n","    images = []\n","    \n","    vidcap = cv2.VideoCapture(in_file)\n","    \n","    success,image = vidcap.read()\n","        \n","    count = 0\n","\n","    while count<_images_per_file:\n","                \n","        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    \n","        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n","                                 interpolation=cv2.INTER_CUBIC)\n","    \n","        images.append(res)\n","    \n","        success,image = vidcap.read()\n","    \n","        count += 1\n","        \n","    resul = np.array(images)\n","    \n","    resul = (resul / 255.).astype(np.float16)\n","        \n","    return resul\n","\n","\n","\n","def get_transfer_values(current_dir, file_name):\n","\n","\n","    # Pre-allocate input-batch-array for images.\n","    shape = (_images_per_file,) + img_size_touple + (3,)\n","    \n","    image_batch = np.zeros(shape=shape, dtype=np.float16)\n","    \n","    image_batch = get_frames(current_dir, file_name)\n","      \n","    # Pre-allocate output-array for transfer-values.\n","    # Note that we use 16-bit floating-points to save memory.\n","    shape = (_images_per_file, transfer_values_size)\n","    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","\n","    transfer_values = \\\n","            loaded_image_model_transfer.predict(image_batch)\n","            \n","    return transfer_values\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkovjuAWoc0r","colab_type":"code","colab":{}},"source":["def violence_predict(filename):\n","  vid_name = filename\n","  x = get_transfer_values(\"/content\", vid_name)\n","  x = np.expand_dims(x,axis=0)\n","  return loaded_model.predict(x)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"caxYzDWzVVcq","colab_type":"code","colab":{}},"source":["import os\n","import shutil\n","\n","def create_folder_for_packets():  \n","  shutil.rmtree('vid2')\n","  path = \"/content/vid2\"\n","  os.mkdir(path)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8g0dya8oo3uj","colab_type":"text"},"source":["# Violence Prediction"]},{"cell_type":"code","metadata":{"id":"NeOYEkdJCOsT","colab_type":"code","colab":{}},"source":["import math\n","from numpy import mean\n","def fVI(img_arr):\n","\n","    AVG = []\n","    img_arr_len = len(img_arr)\n","    if img_arr_len % 20 != 0: \n","        temp_reminder = img_arr_len % 20\n","        last_frames_packet = img_arr[-20 : ]\n","        del img_arr[-temp_reminder : ]\n","\n","\n","\n","    height, width, layers = img_arr[0].shape\n","    size = (width,height)    \n","\n","    len_of_packets = math.floor(img_arr_len / 20)\n","    i = 0\n","    j = 20\n","\n","    \n","    for packet in range(len_of_packets):\n","        res = img_arr[i : j] \n","        i += 20\n","        j += 20\n","        out = cv2.VideoWriter('/content/vid2/temp_violence{}.avi'.format(packet),cv2.VideoWriter_fourcc(*'DIVX'), 20 , size)\n","\n","        for i in range(20):\n","            out.write(res[i])\n","    \n","        out.release()\n","        cv2.destroyAllWindows()\n","\n","\n","    for packet in range(len_of_packets):\n","\n","        violence_prediction = violence_predict('/content/vid2/temp_violence{}.avi'.format(packet))[0][0]\n","        AVG.append(violence_prediction)\n","        \n","\n","    mean_val_of_violence = mean(AVG)\n","\n","    if mean_val_of_violence > 0.80:\n","      print(\"VIOLENCE DETECTED\")\n","    else:\n","      print(\"VIOLENCE NOT DETECTED\")\n","\n","    create_folder_for_packets()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWp-vip1pD0K","colab_type":"text"},"source":["# Fire Preparation\n"]},{"cell_type":"code","metadata":{"id":"DJpmdVKt29Cf","colab_type":"code","colab":{}},"source":["!pip install  imageai --upgrade \n","!wget install https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/fire-dataset.zip\n","!unzip /content/fire-dataset.zip\n","!wget install https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5\n","!wget install https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/detection_model-ex-33--loss-4.97.h5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAfhSWup3Rw9","colab_type":"code","colab":{}},"source":[" %tensorflow_version 1.x\n","from imageai.Detection.Custom import CustomObjectDetection, CustomVideoObjectDetection\n","import os\n","\n","execution_path = os.getcwd()\n","\n","total_fire_frames = 0\n","total_non = 0\n","\n","def forFrame(frame_number, output_array, output_count):\n","    if bool(output_count):\n","      global total_fire_frames \n","      total_fire_frames  += 1\n","    else:\n","      global total_non\n","      total_non += 1  \n","\n","\n","def detect_from_video(filename):\n","    detector = CustomVideoObjectDetection()\n","    detector.setModelTypeAsYOLOv3()\n","    detector.setModelPath(detection_model_path=os.path.join(execution_path, \"detection_model-ex-33--loss-4.97.h5\"))\n","    detector.setJsonPath(configuration_json=os.path.join(execution_path, \"/content/drive/My Drive/Emergency project/FireDetection/detection_config.json\"))\n","    detector.loadModel()\n","    \n","    detected_video_path = detector.detectObjectsFromVideo(input_file_path=os.path.join(execution_path, filename), frames_per_second=30,  minimum_percentage_probability=30, log_progress=False, save_detected_video=False, per_frame_function=forFrame )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TN5tXLKqxQM","colab_type":"text"},"source":["# Fire Prediction"]},{"cell_type":"code","metadata":{"id":"W4R1yDRgC6Nr","colab_type":"code","colab":{}},"source":["def fFI(filename):\n","  detect_from_video(filename)\n","\n","  global total_fire_frames\n","  global total_non\n","\n","  if (total_fire_frames / (total_fire_frames + total_non)) * 100  >= 15:\n","    print(\"FIRE DETECTED\")\n","  else:\n","    print(\"NO FIRE DETECTED\")  \n","\n","  \n","  total_fire_frames -= total_fire_frames\n","  total_non -= total_non\n","\n","  \n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Emk-Rmb9rqk_","colab_type":"text"},"source":["# Accidents Preparation"]},{"cell_type":"code","metadata":{"id":"atBRMf0SmqG_","colab_type":"code","colab":{}},"source":["!wget https://github.com/saifrais/w210-accident-detection/blob/master/inception_v3/retrained_graph.pb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ld4ZNJOm0Jb","colab_type":"code","colab":{}},"source":["\n","import torch \n","from PIL import Image, ImageFile\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data_utils\n","import torchvision\n","from PIL import Image, ImageFile\n","from torch import nn\n","from torch import optim as optim\n","from torch.autograd import Variable\n","from torch.optim import lr_scheduler\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision import datasets, models, transforms\n","from google.colab.patches import cv2_imshow\n","import cv2\n","\n","def accidents_predict(filename):\n","\n","  train_on_gpu = torch.cuda.is_available()\n","  if not train_on_gpu:\n","      print('CUDA is not available.  Training on CPU ...')\n","\n","  # else:\n","  #     print('CUDA is available!  Training on GPU ...')\n","  ImageFile.LOAD_TRUNCATED_IMAGES = True\n","  #!pip install --upgrade wandb\n","\n","\n","\n","\n","  test_transforms = transforms.Compose([transforms.Resize(255),\n","                                        #  transforms.CenterCrop(224),\n","                                        transforms.ToTensor(),\n","                                        ])\n","\n","  model = models.densenet161()\n","\n","\n","  model.classifier = nn.Sequential(nn.Linear(2208, 1000),\n","                                  nn.ReLU(),\n","                                  nn.Dropout(0.2),\n","                                  nn.Linear(1000, 2),\n","                                  nn.LogSoftmax(dim=1))\n","\n","  criterion = nn.NLLLoss()\n","  # Only train the classifier parameters, feature parameters are frozen\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","  model = model.cuda()\n","\n","  model.load_state_dict(torch.load(\"/content/drive/My Drive/Emergency project/AccidentsDetection/tensorboardexp.pt\"))\n","  classes = [\"accident\", \"noaccident\"]\n","  \n","  count = 0\n","  counts = 1\n","  videopath = filename\n","\n","  label_arr = []\n","  \n","\n","  vid = cv2.VideoCapture(videopath)\n","  ret = True\n","  while ret:\n","      if ret == True:\n","          ret, frame = vid.read()\n","\n","          try:\n","              img = Image.fromarray(frame)\n","          except ValueError:\n","              break\n","          except AttributeError:\n","              break\n","          img = test_transforms(img)\n","          img = img.unsqueeze(dim=0)\n","          img = img.cuda()\n","          model.eval()\n","          with torch.no_grad():\n","              output = model(img)\n","              _, predicted = torch.max(output, 1)\n","\n","              index = int(predicted.item())\n","              if index == 0:\n","                  count += 1\n","                  if counts == 1:\n","                      counts += 1\n","\n","              labels = classes[index]\n","\n","\n","          label_arr.append(labels)\n","          \n","\n","          if cv2.waitKey(1) & 0xFF == ord('q'):\n","              break\n","\n","\n","  \n","  return label_arr"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLZ2ZZzWr0y1","colab_type":"text"},"source":["# Accidents Predict"]},{"cell_type":"code","metadata":{"id":"UgCNxBFa75Um","colab_type":"code","colab":{}},"source":["def fAC(filename):\n","\n","  total_accident_frames = 0\n","  total_accident_non = 0\n","\n","  total_accident = accidents_predict(filename)\n","  \n","  for i in total_accident:\n","    if i == \"accident\":\n","      total_accident_frames += 1\n","    else:\n","      total_accident_non += 1\n","\n","  if (total_accident_frames / (total_accident_frames + total_accident_non)) * 100  >= 15:\n","    print(\"ACCIDENT DETECTED\")\n","  else:\n","    print(\"NO ACCIDENT DETECTED\")  \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZlIrHctXs6I","colab_type":"text"},"source":["# ALL Emergency Predictions "]},{"cell_type":"code","metadata":{"id":"94tZqjVxuoKK","colab_type":"code","colab":{}},"source":["from multiprocessing import Process\n","# Please use the path of the video after uploading it.\n","video_path = \"Your Video Path\"\n","img_arr_for_violence, processed_vid = preprocessing(video_path) \n","\n","p1 = Process(target=fFI(processed_vid))\n","p1.start()\n","\n","p2 = Process(target=fAC(processed_vid))\n","p2.start()\n","\n","p3 = Process(target=fVI(img_arr_for_violence))\n","p3.start()\n","\n","\n","p1.join()\n","p2.join()\n","p3.join()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LmLRkdYY1jI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}